---
title: "chapter4.Rmd"
author: "Henrik Wejberg"
date: "2022-11-28"
output: html_document
---


```{r, echo=FALSE}
# This is a code chunk in RStudio editor.
# access the MASS package
library(MASS)
library(tidyverse)

# load the data
data("Boston")

# explore the dataset
str(Boston)
dim(Boston)

# plot matrix of the variables
pairs(Boston)

```

Boston dataset has 14 variables and 506 observations. They are the following, found through [this link](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html): 

CRIM - per capita crime rate by town
ZN - proportion of residential land zoned for lots over 25,000 sq.ft.
INDUS - proportion of non-retail business acres per town.
CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)
NOX - nitric oxides concentration (parts per 10 million)
RM - average number of rooms per dwelling
AGE - proportion of owner-occupied units built prior to 1940
DIS - weighted distances to five Boston employment centres
RAD - index of accessibility to radial highways
TAX - full-value property-tax rate per $10,000
PTRATIO - pupil-teacher ratio by town
B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
LSTAT - % lower status of the population
MEDV - Median value of owner-occupied homes in $1000's

This data can be used to analyse how the different variables affect each other. For example, how does crime and location affect median values of homes? 

```{r, echo=FALSE}
# calculate the correlation matrix and round it
cor_matrix <- cor(Boston) 

# visualize the correlation matrix
library(corrplot)
corrplot(cor_matrix, method="circle")
summary(Boston)

```

Correlation plot shows well what kind of variables have strong connections with each other. For example, index of accessibility to radial highways is positively correlated with non-retail business acres and nox emissions. It is likely that factories are constructed close to highways for ease of transport, which also leads to high emissions in these areas. Summary of the variables is not terribly interesting. It is worth while to point out that chas is a dummy variable, which explains the 0 value in all others than mean. 

```{r, echo = FALSE}
# center and standardize variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)

# class of the boston_scaled object
class(boston_scaled)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)

# summary of the scaled crime rate
summary(boston_scaled$crim)

# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE)

# look at the table of the new factor crime
table(crime)

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

```

We scaled the data so every variable would have a mean of 0. This is done by subtracting from every observation the variable mean of the same variable and dividing by the variables standard deviation. After this, as previously said, all variables have a mean of 0 and all variables have also negative values. Compared to pre-scaling, there wasn't every negative values since home values cannot be negative. 

The later outputs show that we created quantiles for crime. All the observations have been divided equally to four groups, where crime rates differ inside the groups. The last two groups, which also have the most crime, has much more variance than the previous two, which can be seen from the "edge" values of the groups. 

Training and test groups were also created by sampling 80 % of the data to training set and 20 % to test set. 


```{r, echo = FALSE}
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}


# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes)
lda.arrows(lda.fit, myscale = 1) 

```

We use the training set to conduct linear discriminant analysis. We choose crime rate as target variable and assign all other variables in the training set as predictor variables. In group means we see what the mean values of other variables are in certain quantiles of crime levels. For example, there isn't as much crime in industry heavy areas, since variable zn is quite high in the areas where least crime happens. From the lda plot, we can see quite clearly that the areas can be divided to two groups based on the crime rate in different areas. 


```{r, echo = FALSE}

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)

```
Previously, we created two data sets and from the test data set, we removed the crime variable and assigned it to other data frame. We can now use the lda.fit model to predict how well it predicts areas to correct quantiles regarding crime. As we can see from the results, it does the predictions quite well. No error on the clearly higher crime areas which are the last quintile with 31 observations. There is most error between the two first ones which is to be expected, since the quintiles range are so close to each other. 


```{r, echo = FALSE}
set.seed(123)

data("Boston")

Boston <- scale(Boston)

# euclidean distance matrix
dist_eu <- dist(Boston)

# look at the summary of the distances
summary(dist_eu)

# manhattan distance matrix
dist_man <- dist(Boston, method = "manhattan")

# look at the summary of the distances
summary(dist_man)
summary(dist_eu)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

# k-means clustering
km <- kmeans(Boston, centers = 2)

# plot the Boston dataset with clusters
pairs(Boston, col = km$cluster)
```

We calculated the total within sum of squares. From the literature we know that when the sum of squares drastically changes after changing the number of groups, then we should choose the number of groups were the drastic change happened. The greatest reduction is between 1:2, so we decided to use two groups. 

From the second plot we can see how all the variables are divided to two groups. In most of the pairs, the red and black observations are clearly separated from each other. Based on this visual confirmation, we can say that the k-means groups increases our understanding of the data and help us make predictions. 